{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#let's-try-re-coding-the-mnist-to-make-sure-we-know-the-'flow'\" data-toc-modified-id=\"let's-try-re-coding-the-mnist-to-make-sure-we-know-the-'flow'-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>let's try re-coding the mnist to make sure we know the 'flow'</a></span><ul class=\"toc-item\"><li><span><a href=\"#imports-methods\" data-toc-modified-id=\"imports-methods-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>imports methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#create-train,-test-X_data-and-Y_labels\" data-toc-modified-id=\"create-train,-test-X_data-and-Y_labels-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>create train, test X_data and Y_labels</a></span></li><li><span><a href=\"#encoder-time\" data-toc-modified-id=\"encoder-time-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>encoder time</a></span></li><li><span><a href=\"#training-loop\" data-toc-modified-id=\"training-loop-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>training loop</a></span></li></ul></li><li><span><a href=\"#'columns'\" data-toc-modified-id=\"'columns'-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>'columns'</a></span><ul class=\"toc-item\"><li><span><a href=\"#sp.compute(inputVector,-learn,-activeArray)\" data-toc-modified-id=\"sp.compute(inputVector,-learn,-activeArray)-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>sp.compute(inputVector, learn, activeArray)</a></span></li><li><span><a href=\"#SDRClassifier-(sdrc)\" data-toc-modified-id=\"SDRClassifier-(sdrc)-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>SDRClassifier (sdrc)</a></span></li><li><span><a href=\"#testing-loop\" data-toc-modified-id=\"testing-loop-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>testing loop</a></span></li><li><span><a href=\"#something-i-missed-earlier-about-sp.compute():\" data-toc-modified-id=\"something-i-missed-earlier-about-sp.compute():-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>something i missed earlier about sp.compute():</a></span></li><li><span><a href=\"#for-testing:\" data-toc-modified-id=\"for-testing:-1.2.5\"><span class=\"toc-item-num\">1.2.5&nbsp;&nbsp;</span>for testing:</a></span></li></ul></li><li><span><a href=\"#my-current-understanding-of-SpatialPooler-and-'columns'\" data-toc-modified-id=\"my-current-understanding-of-SpatialPooler-and-'columns'-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>my current understanding of SpatialPooler and 'columns'</a></span></li><li><span><a href=\"#hotgym-example\" data-toc-modified-id=\"hotgym-example-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>hotgym example</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's try re-coding the mnist to make sure we know the 'flow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "from htm.bindings.algorithms import SpatialPooler, Classifier\n",
    "from htm.bindings.sdr import SDR, Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ds(name, num_test, shape=None): # get data from openml, split to train test\n",
    "    # name=openML ID, num_test=num_samples for testing, shape=new reshape of datapoint\n",
    "    data = fetch_openml(name, version=1)\n",
    "    sz = data['target'].shape[0]\n",
    "    \n",
    "    X = data['data']\n",
    "    if shape is not None:\n",
    "        new_shape = shape.insert(0, sz)\n",
    "        X = np.reshape(X, shape) # why reshape here?\n",
    "    \n",
    "    y = data['target'].astype(np.int32)\n",
    "    train_labels = y[:sz-num_test]\n",
    "    train_images = X[:sz-num_test]\n",
    "    test_labels  = y[sz-num_test:]\n",
    "    test_images  = X[sz-num_test:]\n",
    "\n",
    "    return train_labels, train_images, test_labels, test_images\n",
    "\n",
    "def encode(data, out):\n",
    "    # encode image data, with raw data + SDR_with_encoded_data\n",
    "    out.dense = data >= np.mean(data) # convert greyscale to binary B/W\n",
    "    # works for MNIST but loses on color images\n",
    "    # this is the encoder, so i should think more on how it's going\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create train, test X_data and Y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These parameters can be improved using parameter optimization,\n",
    "# see py/htm/optimization/ae.py\n",
    "# For more explanation of relations between the parameters, see \n",
    "# src/examples/mnist/MNIST_CPP.cpp \n",
    "default_parameters = {\n",
    "    'potentialRadius': 7,\n",
    "    'boostStrength': 7.0,\n",
    "    'columnDimensions': (79, 79),\n",
    "    'dutyCyclePeriod': 1402,\n",
    "    'localAreaDensity': 0.1,\n",
    "    'minPctOverlapDutyCycle': 0.2,\n",
    "    'potentialPct': 0.1,\n",
    "    'stimulusThreshold': 6,\n",
    "    'synPermActiveInc': 0.14,\n",
    "    'synPermConnected': 0.5,\n",
    "    'synPermInactiveDec': 0.02\n",
    "}\n",
    "train_labels, train_images, test_labels, test_images = load_ds('mnist_784', \n",
    "                                                               #HTM: ~95% Fashion-MNIST\n",
    "                                                              10000, shape=[28,28]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip imgs and labels together for train and test (zipped list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_data = list(zip(train_images, train_labels))\n",
    "test_data = list(zip(test_images, test_labels))\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[2][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that makes more sense. stored as a long list of tuples, where the first element is an array, second is integer (since we're classifying integers, it's the label to the first element).\n",
    "\n",
    "[ (image, label) ... ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = SDR(train_images[0].shape) # create encoder with needed shape\n",
    "parameters = default_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "htm.bindings.sdr.SDR"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(enc.dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc.dense) # riiight it's a matching shape for the input array, the SP is much bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------CPP SpatialPooler Parameters ------------------\n",
      "iterationNum                = 0\n",
      "iterationLearnNum           = 0\n",
      "numInputs                   = 784\n",
      "numColumns                  = 6241\n",
      "numActiveColumnsPerInhArea  = 0\n",
      "potentialPct                = 0.1\n",
      "globalInhibition            = 1\n",
      "localAreaDensity            = 0.1\n",
      "stimulusThreshold           = 6\n",
      "synPermActiveInc            = 0.14\n",
      "synPermInactiveDec          = 0.02\n",
      "synPermConnected            = 0.5\n",
      "minPctOverlapDutyCycles     = 0.2\n",
      "dutyCyclePeriod             = 1402\n",
      "boostStrength               = 7\n",
      "spVerbosity                 = 99\n",
      "wrapAround                  = 0\n",
      "version                     = 2\n",
      "CPP SP seed                 = 0\n"
     ]
    }
   ],
   "source": [
    "sp = SpatialPooler( # create SP with lots of pre-set params\n",
    "    inputDimensions            = enc.dimensions,\n",
    "    columnDimensions           = parameters['columnDimensions'],\n",
    "    potentialRadius            = parameters['potentialRadius'],\n",
    "    potentialPct               = parameters['potentialPct'],\n",
    "    globalInhibition           = True,\n",
    "    localAreaDensity           = parameters['localAreaDensity'],\n",
    "    stimulusThreshold          = int(round(parameters['stimulusThreshold'])),\n",
    "    synPermInactiveDec         = parameters['synPermInactiveDec'],\n",
    "    synPermActiveInc           = parameters['synPermActiveInc'],\n",
    "    synPermConnected           = parameters['synPermConnected'],\n",
    "    minPctOverlapDutyCycle     = parameters['minPctOverlapDutyCycle'],\n",
    "    dutyCyclePeriod            = int(round(parameters['dutyCyclePeriod'])),\n",
    "    boostStrength              = parameters['boostStrength'],\n",
    "    seed                       = 0, # this is important, 0=\"random\" seed which changes on each invocation\n",
    "    spVerbosity                = 99,\n",
    "    wrapAround                 = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = SDR(sp.getColumnDimensions()) # create new sdr with shape of SP columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = sp.getColumnDimensions()\n",
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[79, 79]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d # so our spatial pooler output is a len=6241 SDR? it seems so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "htm.bindings.sdr.Metrics"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_stats = Metrics(columns, 99999999)\n",
    "type(columns_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdrc = Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data, out):\n",
    "    # encode image data, with raw data + SDR_with_encoded_data\n",
    "    out.dense = data >= np.mean(data) # convert greyscale to binary B/W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial Pooler Connections:\n",
      "    Inputs (784) ~> Outputs (6241) via Segments (6241)\n",
      "    Segments on Cell Min/Mean/Max 1 / 1 / 1\n",
      "    Potential Synapses on Segment Min/Mean/Max 6 / 17.1349 / 23\n",
      "    Connected Synapses on Segment Min/Mean/Max 6 / 10.6308 / 23\n",
      "    Synapses Dead (0.321604%) Saturated (0.415274%)\n",
      "    Synapses pruned (0%) Segments pruned (0%)\n",
      "    Buffer for destroyed synapses: 0    Buffer for destroyed segments: 0\n",
      "\n",
      "SDR( 79 79 )\n",
      "    Sparsity Min/Mean/Std/Max 0 / 0.077209 / 0.0326378 / 0.099984\n",
      "\r",
      "    Activation Frequency Min/Mean/Std/Max 0 / 0.0772095 / 0.0569753 / 0.212116\n",
      "\r",
      "    Entropy 0.902479\n",
      "\r",
      "    Overlap Min/Mean/Std/Max 0 / 0.0939301 / 0.084383 / 0.639423\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_images)):\n",
    "    img, lbl = training_data[i] # assign 2 variables for each tuple in list\n",
    "    # encode is a simple line so no need to encapsulate a function\n",
    "    enc.dense = img >= np.mean(img) # this does... what, exactly?\n",
    "    sp.compute(enc, True, columns) # should check docs for sp.compute\n",
    "    # http://nupic.docs.numenta.org/stable/api/algorithms/spatial-pooling.html?highlight=spatialpooler#nupic.algorithms.spatial_pooler.SpatialPooler.compute\n",
    "    sdrc.learn(columns, lbl)\n",
    "print(str(sp))\n",
    "print(str(columns_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'columns'\n",
    "- seems to assist the spatial pooler in training, since we use it for sp.compute and sdrc.learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[79, 79]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns.dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "htm.bindings.algorithms.Classifier"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sdrc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sp.compute(inputVector, learn, activeArray)\n",
    "- inputVector\n",
    "    - the SDR from the encodr, feed into SP\n",
    "        - it's just a bit array of matching length, dimensions don't matter it's all a list/array\n",
    "- learn\n",
    "    - boolean: whether or not to learn from this data\n",
    "        - updates the permanence values of synapses\n",
    "        - there's good reasons to turn this off when testing new encoders / data\n",
    "- activeArray\n",
    "    - \"An array whose size is equal to the number of columns. Before the function returns this array will be populated with 1’s at the indices of the active columns, and 0’s everywhere else.\"\n",
    "        - so in our 79x79 SP, this is \"columns\". len(columns.dense) = 79, makes sense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ah blimey, the documentation of nupic & HTM_community are different, i forgot.\n",
    "- sp.compute() and nupic's SDRC.compute() seem to do the same thing, since they have 'learn=True' flag as well.\n",
    "- i'm guessing that htm.sdrc.learn() is the same as nupic.sdrc.infer()\n",
    "    - they both have 2 parameters\n",
    "    - infer(patternNZ, classification)\n",
    "        - patternNZ\n",
    "            - list of active indices from output below\n",
    "                - isn't this the same as our 'columns'?\n",
    "        - classification\n",
    "            - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDRClassifier (sdrc)\n",
    "- single layer classification network\n",
    "    - takes SDRs as input, outputs predicted distribution of classes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__score__:   95.57 %\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "for img, lbl in test_data:\n",
    "#     encode(img, enc)\n",
    "    enc.dense = img >= np.mean(img) # comparing image vs its mean? isn't this a boolean?! i'm missing something big\n",
    "    sp.compute(enc, False, columns)\n",
    "    if lbl == np.argmax(sdrc.infer(columns)):# if lbl = index of max of column-inference\n",
    "        score += 1\n",
    "score = score / len(test_data)\n",
    "print('__score__:  ', 100*score, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "htm.bindings.algorithms.Connections"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sp.connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6241"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.connections.numCells()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### something i missed earlier about sp.compute():\n",
    "- \"This is the primary public method of the SpatialPooler class. This function takes a input vector and outputs the indices of the active columns. If ‘learn’ is set to True, this method also updates the permanences of the columns.\"\n",
    "    - \"updates the permanences of the columns\" explains why the testing loop doesn't manually update \"columns\" variable\n",
    "    - sp.compute updates it as long as you pass it in as the parameter\n",
    "    - 'enc' is altered in each iteration with the encode recent image function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for testing:\n",
    "- for each img, label in test data:\n",
    "    - encode the image with that magic line involving .dense\n",
    "    - compute the encoder through spatial pooler, not learning, using 'columns'\n",
    "    - if label = index of max of column-inference\n",
    "        - got this classification right, add to score\n",
    "        - so what does this actually mean?\n",
    "            - if lbl (0-9 integer) == np.argmax("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## my current understanding of SpatialPooler and 'columns'\n",
    "- SpatialPooler is a fancier SDR with many parameters baked in\n",
    "    - but we can also call sp.connections.numCells() etc\n",
    "    - so sp is the actual \"model\" that's learning\n",
    "        - 'columns' is a 79x79 SDR\n",
    "        - i'm pretty sure 'columns' is like a blank slate in each iteration\n",
    "            - we map the encoded_SDR into 'columns_SDR' which is the size of SP\n",
    "            - and then we use columns to update permanances of main SP\n",
    "\n",
    "that... sounds like it makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = sdrc.infer(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3494252474228923e-05,\n",
       " 1.71701294153338e-07,\n",
       " 0.0014333445666434165,\n",
       " 1.2114306201339948e-06,\n",
       " 8.710573415095605e-05,\n",
       " 0.0005013589760835132,\n",
       " 0.9979530834253529,\n",
       " 9.630532291627714e-07,\n",
       " 6.786450101425321e-06,\n",
       " 2.4660221616319366e-06]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oh that's really elegant, actually\n",
    "- sdrc.infer(columns) returns a list of 10 \"certainties\", one for each class\n",
    "    - np.argmax() returns the indices of the maximum value in an array\n",
    "        - since we order the classes as rising integers 0-9,\n",
    "        - we just check if int(label) == index_of_highest_certainty\n",
    "            - that's really slick\n",
    "another reminder: when confused, just explore the types and properties of each variable, line-by-line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hotgym example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from htm.bindings.sdr import SDR, Metrics\n",
    "from htm.encoders.rdse import RDSE, RDSE_Parameters\n",
    "from htm.encoders.date import DateEncoder\n",
    "from htm.bindings.algorithms import SpatialPooler\n",
    "from htm.bindings.algorithms import TemporalMemory\n",
    "from htm.algorithms.anomaly_likelihood import AnomalyLikelihood #FIXME use TM.anomaly instead, but it gives worse results than the py.AnomalyLikelihood now\n",
    "from htm.bindings.algorithms import Predictor\n",
    "\n",
    "_EXAMPLE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "_INPUT_FILE_PATH = os.path.join(_EXAMPLE_DIR, \"gymdata.csv\")\n",
    "\n",
    "default_parameters = {\n",
    "  # there are 2 (3) encoders: \"value\" (RDSE) & \"time\" (DateTime weekend, timeOfDay)\n",
    " 'enc': {\n",
    "      \"value\" :\n",
    "         {'resolution': 0.88, 'size': 700, 'sparsity': 0.02},\n",
    "      \"time\": \n",
    "         {'timeOfDay': (30, 1), 'weekend': 21}\n",
    " },\n",
    " 'predictor': {'sdrc_alpha': 0.1},\n",
    " 'sp': {'boostStrength': 3.0,\n",
    "        'columnCount': 1638,\n",
    "        'localAreaDensity': 0.04395604395604396,\n",
    "        'potentialPct': 0.85,\n",
    "        'synPermActiveInc': 0.04,\n",
    "        'synPermConnected': 0.13999999999999999,\n",
    "        'synPermInactiveDec': 0.006},\n",
    " 'tm': {'activationThreshold': 17,\n",
    "        'cellsPerColumn': 13,\n",
    "        'initialPerm': 0.21,\n",
    "        'maxSegmentsPerCell': 128,\n",
    "        'maxSynapsesPerSegment': 64,\n",
    "        'minThreshold': 10,\n",
    "        'newSynapseCount': 32,\n",
    "        'permanenceDec': 0.1,\n",
    "        'permanenceInc': 0.1},\n",
    " 'anomaly': {\n",
    "   'likelihood': \n",
    "       {#'learningPeriod': int(math.floor(self.probationaryPeriod / 2.0)),\n",
    "        #'probationaryPeriod': self.probationaryPeriod-default_parameters[\"anomaly\"][\"likelihood\"][\"learningPeriod\"],\n",
    "        'probationaryPct': 0.1,\n",
    "        'reestimationPeriod': 100} #These settings are copied from NAB\n",
    " }\n",
    "}\n",
    "\n",
    "def main(parameters=default_parameters, argv=None, verbose=True):\n",
    "  if verbose:\n",
    "    import pprint\n",
    "    print(\"Parameters:\")\n",
    "    pprint.pprint(parameters, indent=4)\n",
    "    print(\"\")\n",
    "\n",
    "  # Read the input file.\n",
    "  records = []\n",
    "  with open(_INPUT_FILE_PATH, \"r\") as fin:\n",
    "    reader = csv.reader(fin)\n",
    "    headers = next(reader)\n",
    "    next(reader)\n",
    "    next(reader)\n",
    "    for record in reader:\n",
    "      records.append(record)\n",
    "\n",
    "  # Make the Encoders.  These will convert input data into binary representations.\n",
    "  dateEncoder = DateEncoder(timeOfDay= parameters[\"enc\"][\"time\"][\"timeOfDay\"], \n",
    "                            weekend  = parameters[\"enc\"][\"time\"][\"weekend\"]) \n",
    "  \n",
    "  scalarEncoderParams            = RDSE_Parameters()\n",
    "  scalarEncoderParams.size       = parameters[\"enc\"][\"value\"][\"size\"]\n",
    "  scalarEncoderParams.sparsity   = parameters[\"enc\"][\"value\"][\"sparsity\"]\n",
    "  scalarEncoderParams.resolution = parameters[\"enc\"][\"value\"][\"resolution\"]\n",
    "  scalarEncoder = RDSE( scalarEncoderParams )\n",
    "  encodingWidth = (dateEncoder.size + scalarEncoder.size)\n",
    "  enc_info = Metrics( [encodingWidth], 999999999 )\n",
    "\n",
    "  # Make the HTM.  SpatialPooler & TemporalMemory & associated tools.\n",
    "  spParams = parameters[\"sp\"]\n",
    "  sp = SpatialPooler(\n",
    "    inputDimensions            = (encodingWidth,),\n",
    "    columnDimensions           = (spParams[\"columnCount\"],),\n",
    "    potentialPct               = spParams[\"potentialPct\"],\n",
    "    potentialRadius            = encodingWidth,\n",
    "    globalInhibition           = True,\n",
    "    localAreaDensity           = spParams[\"localAreaDensity\"],\n",
    "    synPermInactiveDec         = spParams[\"synPermInactiveDec\"],\n",
    "    synPermActiveInc           = spParams[\"synPermActiveInc\"],\n",
    "    synPermConnected           = spParams[\"synPermConnected\"],\n",
    "    boostStrength              = spParams[\"boostStrength\"],\n",
    "    wrapAround                 = True\n",
    "  )\n",
    "  sp_info = Metrics( sp.getColumnDimensions(), 999999999 )\n",
    "\n",
    "  tmParams = parameters[\"tm\"]\n",
    "  tm = TemporalMemory(\n",
    "    columnDimensions          = (spParams[\"columnCount\"],),\n",
    "    cellsPerColumn            = tmParams[\"cellsPerColumn\"],\n",
    "    activationThreshold       = tmParams[\"activationThreshold\"],\n",
    "    initialPermanence         = tmParams[\"initialPerm\"],\n",
    "    connectedPermanence       = spParams[\"synPermConnected\"],\n",
    "    minThreshold              = tmParams[\"minThreshold\"],\n",
    "    maxNewSynapseCount        = tmParams[\"newSynapseCount\"],\n",
    "    permanenceIncrement       = tmParams[\"permanenceInc\"],\n",
    "    permanenceDecrement       = tmParams[\"permanenceDec\"],\n",
    "    predictedSegmentDecrement = 0.0,\n",
    "    maxSegmentsPerCell        = tmParams[\"maxSegmentsPerCell\"],\n",
    "    maxSynapsesPerSegment     = tmParams[\"maxSynapsesPerSegment\"]\n",
    "  )\n",
    "  tm_info = Metrics( [tm.numberOfCells()], 999999999 )\n",
    "\n",
    "  # setup likelihood, these settings are used in NAB\n",
    "  anParams = parameters[\"anomaly\"][\"likelihood\"]\n",
    "  probationaryPeriod = int(math.floor(float(anParams[\"probationaryPct\"])*len(records)))\n",
    "  learningPeriod     = int(math.floor(probationaryPeriod / 2.0))\n",
    "  anomaly_history = AnomalyLikelihood(learningPeriod= learningPeriod,\n",
    "                                      estimationSamples= probationaryPeriod - learningPeriod,\n",
    "                                      reestimationPeriod= anParams[\"reestimationPeriod\"])\n",
    "\n",
    "  predictor = Predictor( steps=[1, 5], alpha=parameters[\"predictor\"]['sdrc_alpha'] )\n",
    "  predictor_resolution = 1\n",
    "\n",
    "  # Iterate through every datum in the dataset, record the inputs & outputs.\n",
    "  inputs      = []\n",
    "  anomaly     = []\n",
    "  anomalyProb = []\n",
    "  predictions = {1: [], 5: []}\n",
    "  for count, record in enumerate(records):\n",
    "\n",
    "    # Convert date string into Python date object.\n",
    "    dateString = datetime.datetime.strptime(record[0], \"%m/%d/%y %H:%M\")\n",
    "    # Convert data value string into float.\n",
    "    consumption = float(record[1])\n",
    "    inputs.append( consumption )\n",
    "\n",
    "    # Call the encoders to create bit representations for each value.  These are SDR objects.\n",
    "    dateBits        = dateEncoder.encode(dateString)\n",
    "    consumptionBits = scalarEncoder.encode(consumption)\n",
    "\n",
    "    # Concatenate all these encodings into one large encoding for Spatial Pooling.\n",
    "    encoding = SDR( encodingWidth ).concatenate([consumptionBits, dateBits])\n",
    "    enc_info.addData( encoding )\n",
    "\n",
    "    # Create an SDR to represent active columns, This will be populated by the\n",
    "    # compute method below. It must have the same dimensions as the Spatial Pooler.\n",
    "    activeColumns = SDR( sp.getColumnDimensions() )\n",
    "\n",
    "    # Execute Spatial Pooling algorithm over input space.\n",
    "    sp.compute(encoding, True, activeColumns)\n",
    "    sp_info.addData( activeColumns )\n",
    "\n",
    "    # Execute Temporal Memory algorithm over active mini-columns.\n",
    "    tm.compute(activeColumns, learn=True)\n",
    "    tm_info.addData( tm.getActiveCells().flatten() )\n",
    "\n",
    "    # Predict what will happen, and then train the predictor based on what just happened.\n",
    "    pdf = predictor.infer( tm.getActiveCells() )\n",
    "    for n in (1, 5):\n",
    "      if pdf[n]:\n",
    "        predictions[n].append( np.argmax( pdf[n] ) * predictor_resolution )\n",
    "      else:\n",
    "        predictions[n].append(float('nan'))\n",
    "\n",
    "    anomalyLikelihood = anomaly_history.anomalyProbability( consumption, tm.anomaly )\n",
    "    anomaly.append( tm.anomaly )\n",
    "    anomalyProb.append( anomalyLikelihood )\n",
    "\n",
    "    predictor.learn(count, tm.getActiveCells(), int(consumption / predictor_resolution))\n",
    "\n",
    "\n",
    "  # Print information & statistics about the state of the HTM.\n",
    "  print(\"Encoded Input\", enc_info)\n",
    "  print(\"\")\n",
    "  print(\"Spatial Pooler Mini-Columns\", sp_info)\n",
    "  print(str(sp))\n",
    "  print(\"\")\n",
    "  print(\"Temporal Memory Cells\", tm_info)\n",
    "  print(str(tm))\n",
    "  print(\"\")\n",
    "\n",
    "  # Shift the predictions so that they are aligned with the input they predict.\n",
    "  for n_steps, pred_list in predictions.items():\n",
    "    for x in range(n_steps):\n",
    "        pred_list.insert(0, float('nan'))\n",
    "        pred_list.pop()\n",
    "\n",
    "  # Calculate the predictive accuracy, Root-Mean-Squared\n",
    "  accuracy         = {1: 0, 5: 0}\n",
    "  accuracy_samples = {1: 0, 5: 0}\n",
    "\n",
    "  for idx, inp in enumerate(inputs):\n",
    "    for n in predictions: # For each [N]umber of time steps ahead which was predicted.\n",
    "      val = predictions[n][ idx ]\n",
    "      if not math.isnan(val):\n",
    "        accuracy[n] += (inp - val) ** 2\n",
    "        accuracy_samples[n] += 1\n",
    "  for n in sorted(predictions):\n",
    "    accuracy[n] = (accuracy[n] / accuracy_samples[n]) ** .5\n",
    "    print(\"Predictive Error (RMS)\", n, \"steps ahead:\", accuracy[n])\n",
    "\n",
    "  # Show info about the anomaly (mean & std)\n",
    "  print(\"Anomaly Mean\", np.mean(anomaly))\n",
    "  print(\"Anomaly Std \", np.std(anomaly))\n",
    "\n",
    "  # Plot the Predictions and Anomalies.\n",
    "  if verbose:\n",
    "    try:\n",
    "      import matplotlib.pyplot as plt\n",
    "    except:\n",
    "      print(\"WARNING: failed to import matplotlib, plots cannot be shown.\")\n",
    "      return -accuracy[5]\n",
    "\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title(\"Predictions\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Power Consumption\")\n",
    "    plt.plot(np.arange(len(inputs)), inputs, 'red',\n",
    "             np.arange(len(inputs)), predictions[1], 'blue',\n",
    "             np.arange(len(inputs)), predictions[5], 'green',)\n",
    "    plt.legend(labels=('Input', '1 Step Prediction, Shifted 1 step', '5 Step Prediction, Shifted 5 steps'))\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.title(\"Anomaly Score\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Power Consumption\")\n",
    "    inputs = np.array(inputs) / max(inputs)\n",
    "    plt.plot(np.arange(len(inputs)), inputs, 'red',\n",
    "             np.arange(len(inputs)), anomaly, 'blue',)\n",
    "    plt.legend(labels=('Input', 'Anomaly Score'))\n",
    "    plt.show()\n",
    "\n",
    "  return -accuracy[5]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "47.429px",
    "left": "929.93px",
    "top": "52.5142px",
    "width": "160.98px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
