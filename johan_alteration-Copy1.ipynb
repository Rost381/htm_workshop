{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#original-hotgym-post-training-loop\" data-toc-modified-id=\"original-hotgym-post-training-loop-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>original hotgym post-training loop</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying out Johan Wider's conversion of NuPIC Predictor to community core edition.\n",
    "https://discourse.numenta.org/t/predictor-learn-before-infer-errors-on-hotgym/7767/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htm.bindings.sdr import SDR, Metrics\n",
    "from htm.encoders.rdse import RDSE, RDSE_Parameters\n",
    "from htm.encoders.date import DateEncoder\n",
    "from htm.bindings.algorithms import SpatialPooler\n",
    "from htm.bindings.algorithms import TemporalMemory\n",
    "from htm.algorithms.anomaly_likelihood import AnomalyLikelihood #FIXME use TM.anomaly instead, but it gives worse results than the py.AnomalyLikelihood now\n",
    "from htm.bindings.algorithms import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htm.bindings.algorithms import Predictor\n",
    "from htm.bindings.sdr import SDR, Metrics\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hotgym predictor looks like:\n",
    "predictor = Predictor(steps=[1,5], alpha=parameters['predictor']['sdrc_alpha'])\n",
    "predictor_resolution = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDR( 2048, 1 ) 602, 1449, 1810, 2009\n",
      "SDR( 2048, 1 ) 377, 570, 901, 2005\n",
      "SDR( 2048, 1 ) 87, 232, 516, 1270\n",
      "Prediction for sequence 0\n",
      "[0.00278874038341076, 0.00278874038341076, 0.00278874038341076, 0.00278874038341076, 0.00278874038341076, 0.9748598972891958, 0.002799143516238742, 0.002799143516238742, 0.002799143516238742, 0.002799143516238742]\n",
      "5\n",
      "Prediction for sequence 1\n",
      "[0.0027931382581211184, 0.0027931382581211184, 0.0027931382581211184, 0.0027931382581211184, 0.0027931382581211184, 0.0027931382581211184, 0.0027931382581211184, 0.0027931382581211184, 0.0027931382581211184, 0.9748617099448095]\n",
      "9\n",
      "Prediction for sequence 2\n",
      "[0.0027961843766260383, 0.9748339543943719, 0.0027961843766260383, 0.0027961843766260383, 0.0027961843766260383, 0.0027961843766260383, 0.0027961843766260383, 0.0027961843766260383, 0.0027961843766260383, 0.0027961843766260383]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Try varying the number of steps and alpha\n",
    "num_of_steps = 1\n",
    "alpha = 0.01\n",
    "\n",
    "predictor = Predictor( steps=[num_of_steps], alpha=alpha )\n",
    "#classifier = SDRClassifierFactory.create(steps=[num_of_steps], alpha=alpha)\n",
    "\n",
    "# Make a sequence of SDRs\n",
    "sequence = []\n",
    "sdr = SDR((2048, 1))\n",
    "sdr.sparse = [602, 1449, 1810, 2009]\n",
    "sequence.append(sdr)\n",
    "sdr = SDR((2048, 1))\n",
    "sdr.sparse = [377, 570, 901, 2005]\n",
    "sequence.append(sdr)\n",
    "sdr = SDR((2048, 1))\n",
    "sdr.sparse = [87, 232, 516, 1270]\n",
    "sequence.append(sdr)\n",
    "\n",
    "labels = [1, 5, 9]      # Category labels for the sequence\n",
    "\n",
    "print(sequence[0])\n",
    "print(sequence[1])\n",
    "print(sequence[2])\n",
    "\n",
    "type(sequence)\n",
    "\n",
    "sequence[0].sparse # sparse returns the non-0 values in an SDR.\n",
    "# there's probably some way to return their indices and zip the lists\n",
    "\n",
    "\n",
    "# TRAINING LOOP\n",
    "# all they do is call predictor.learn()\n",
    "# in hotgym training loop, run each datapiece through encoders,\n",
    "# then SP.compute(), then TM.compute()\n",
    "# and we use predictor.learn(). but they call predictor.infer() before that\n",
    "# why?\n",
    "N = 1000\n",
    "predictor.reset()\n",
    "for i in range(3*N):\n",
    "    predictor.learn( i, sequence[i%3], labels[i%3])\n",
    "#     print(i, type(i))\n",
    "#     print(sequence[i%3], type(sequence[i%3]))\n",
    "#     print(labels[i%3], type(labels[i%3]))\n",
    "#     print()\n",
    "\n",
    "# - so it's feeding Predictor.learn() an int, an SDR, and another int\n",
    "#     - it crashes when we feed it a float. so i'm thinking of this wrong, it's a classifier not a float-continuous output predictor... maybe\n",
    "\n",
    "# so the training loop, using predictor, is something like:\n",
    "# - for each data+label:\n",
    "#     - predictor.learn(record_iterator_number, data, label)\n",
    "#     - predictor.infer() is used to present a prediction after training.\n",
    "\n",
    "# Give the predictor partial information, and make predictions\n",
    "predictor.reset()\n",
    "a = predictor.infer( sequence[0] )\n",
    "print(\"Prediction for sequence 0\")\n",
    "print( a[1] )\n",
    "print(np.argmax( a[1] ) )\n",
    "\n",
    "a = predictor.infer( sequence[1] )\n",
    "print(\"Prediction for sequence 1\")\n",
    "print( a[1] )\n",
    "print(np.argmax( a[1] ) )\n",
    "\n",
    "a = predictor.infer( sequence[2] )\n",
    "print(\"Prediction for sequence 2\")\n",
    "print( a[1] )\n",
    "print(np.argmax( a[1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [0.0006479261565283467,\n",
       "  0.9941687482689527,\n",
       "  0.0006479261565283467,\n",
       "  0.0006479261565283467,\n",
       "  0.0006479261565283467,\n",
       "  0.0006479261565283467,\n",
       "  0.0006479261565283467,\n",
       "  0.0006479261565283467,\n",
       "  0.0006479261565283467,\n",
       "  0.0006479261565283467]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original hotgym post-training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  for count, record in enumerate(records):\n",
    "\n",
    "    # Convert date string into Python date object.\n",
    "    dateString = datetime.datetime.strptime(record[0], \"%m/%d/%y %H:%M\")\n",
    "    # Convert data value string into float.\n",
    "    consumption = float(record[1])\n",
    "    inputs.append( consumption )\n",
    "\n",
    "    # Call the encoders to create bit representations for each value.  These are SDR objects.\n",
    "    dateBits        = dateEncoder.encode(dateString)\n",
    "    consumptionBits = scalarEncoder.encode(consumption)\n",
    "\n",
    "    # Concatenate all these encodings into one large encoding for Spatial Pooling.\n",
    "    encoding = SDR( encodingWidth ).concatenate([consumptionBits, dateBits])\n",
    "    enc_info.addData( encoding )\n",
    "\n",
    "    # Create an SDR to represent active columns, This will be populated by the\n",
    "    # compute method below. It must have the same dimensions as the Spatial Pooler.\n",
    "    activeColumns = SDR( sp.getColumnDimensions() )\n",
    "\n",
    "    # Execute Spatial Pooling algorithm over input space.\n",
    "    sp.compute(encoding, True, activeColumns)\n",
    "    sp_info.addData( activeColumns )\n",
    "\n",
    "    # Execute Temporal Memory algorithm over active mini-columns.\n",
    "    tm.compute(activeColumns, learn=True)\n",
    "    tm_info.addData( tm.getActiveCells().flatten() )\n",
    "\n",
    "    # Predict what will happen, and then train the predictor based on what just happened.\n",
    "    pdf = predictor.infer( tm.getActiveCells() )\n",
    "    for n in (1, 5):\n",
    "      if pdf[n]:\n",
    "        predictions[n].append( np.argmax( pdf[n] ) * predictor_resolution )\n",
    "      else:\n",
    "        predictions[n].append(float('nan'))\n",
    "\n",
    "    anomalyLikelihood = anomaly_history.anomalyProbability( consumption, tm.anomaly )\n",
    "    anomaly.append( tm.anomaly )\n",
    "    anomalyProb.append( anomalyLikelihood )\n",
    "\n",
    "    predictor.learn(count, tm.getActiveCells(), int(consumption / predictor_resolution))\n",
    "# Print information & statistics about the state of the HTM.\n",
    "print(\"Encoded Input\", enc_info)\n",
    "print(\"\")\n",
    "print(\"Spatial Pooler Mini-Columns\", sp_info)\n",
    "print(str(sp))\n",
    "print(\"\")\n",
    "print(\"Temporal Memory Cells\", tm_info)\n",
    "print(str(tm))\n",
    "print(\"\"\n",
    "# Shift the predictions so that they are aligned with the input they predict.\n",
    "for n_steps, pred_list in predictions.items():\n",
    "  for x in range(n_steps):\n",
    "      pred_list.insert(0, float('nan'))\n",
    "      pred_list.pop(\n",
    "# Calculate the predictive accuracy, Root-Mean-Squared\n",
    "accuracy         = {1: 0, 5: 0}\n",
    "accuracy_samples = {1: 0, 5: 0\n",
    "for idx, inp in enumerate(inputs):\n",
    "  for n in predictions: # For each [N]umber of time steps ahead which was predicted.\n",
    "    val = predictions[n][ idx ]\n",
    "    if not math.isnan(val):\n",
    "      accuracy[n] += (inp - val) ** 2\n",
    "      accuracy_samples[n] += 1\n",
    "for n in sorted(predictions):\n",
    "  accuracy[n] = (accuracy[n] / accuracy_samples[n]) ** .5\n",
    "  print(\"Predictive Error (RMS)\", n, \"steps ahead:\", accuracy[n]\n",
    "# Show info about the anomaly (mean & std)\n",
    "print(\"Anomaly Mean\", np.mean(anomaly))\n",
    "print(\"Anomaly Std \", np.std(anomaly)\n",
    "# Plot the Predictions and Anomalies.\n",
    "if verbose:\n",
    "  try:\n",
    "    import matplotlib.pyplot as plt\n",
    "  except:\n",
    "    print(\"WARNING: failed to import matplotlib, plots cannot be shown.\")\n",
    "    return -accuracy[5\n",
    "  plt.subplot(2,1,1)\n",
    "  plt.title(\"Predictions\")\n",
    "  plt.xlabel(\"Time\")\n",
    "  plt.ylabel(\"Power Consumption\")\n",
    "  plt.plot(np.arange(len(inputs)), inputs, 'red',\n",
    "           np.arange(len(inputs)), predictions[1], 'blue',\n",
    "           np.arange(len(inputs)), predictions[5], 'green',)\n",
    "  plt.legend(labels=('Input', '1 Step Prediction, Shifted 1 step', '5 Step Prediction, Shifted 5 steps')\n",
    "  plt.subplot(2,1,2)\n",
    "  plt.title(\"Anomaly Score\")\n",
    "  plt.xlabel(\"Time\")\n",
    "  plt.ylabel(\"Power Consumption\")\n",
    "  inputs = np.array(inputs) / max(inputs)\n",
    "  plt.plot(np.arange(len(inputs)), inputs, 'red',\n",
    "           np.arange(len(inputs)), anomaly, 'blue',)\n",
    "  plt.legend(labels=('Input', 'Anomaly Score'))\n",
    "  plt.show(\n",
    "return -accuracy[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
