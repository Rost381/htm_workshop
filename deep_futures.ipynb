{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#predicting-states-beyond-t+1\" data-toc-modified-id=\"predicting-states-beyond-t+1-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>predicting states beyond t+1</a></span><ul class=\"toc-item\"><li><span><a href=\"#&quot;Temporal-Unfolding&quot;\" data-toc-modified-id=\"&quot;Temporal-Unfolding&quot;-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>\"Temporal Unfolding\"</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "for x in range(1):\n",
    "    print(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicting states beyond t+1\n",
    "The scalar predictors I've worked through so far predict outputs at \"t+1\". They predict 'what comes immediately next' and return one time-state to do so.\n",
    "\n",
    "I'm awfully curious about returning t+2, t+3, so on, because:\n",
    "- It should be possible, since we have TM looking at t-1, t-... -> t-cells_per_column\n",
    "- It's been done! Intelletic looks 15 'bars' into the future, as per their product description, and I interpreted this as 'looking 15 states into the future'.\n",
    "    - this makes a lot of sense: you don't just want the stock price t+1, or the next 5 minutes from now. You want a _range_ and _probability_, which the anomaly-categorization model does quite nicely.\n",
    "- Our brains don't just look at one timestate in the future. Or do they? See below post.\n",
    "    - When you see 'football incoming towards my face', your brain outputs 'collision imminent' and associates it with 'pain', which kicks off 'avoid football' routine.\n",
    "        - is this your brain predicting several states (football approaching, impact, pain) after t=0? or running several consecutive calculations, each taking the former output as the new input?\n",
    "        - something tells me it's the second\n",
    "\n",
    "So how exactly is it done? I stumbled on a thread on the forums detailing one example (https://discourse.numenta.org/t/how-temporal-pooler-make-t-n-predictions-if-his-operate-only-t-t-1-timeframes/7826/2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "the TM algorithm alone is only able to make a prediction one time step into the future. This is to match how the connections work in biology. But to make HTM useful in practical ML applications, a lot of non-biological components are thrown into the mix. One of these is the ability to predict further into the future that t+1.\n",
    "\n",
    "There are a few different ways to implement this. One way is to hold a table of running averages on each cell for activity at “t+n” after that cell was active (where “n” is determined ahead of time). Since each cell is part of a high-order context, this works quite well (you are NOT asking “what is most likely to happen 25 timesteps after note G”, but something much more specific like \"what is most likely to happen 25 timesteps after the 357th note of Bethoven’s 5th.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'table of running averages on each cell', contains:\n",
    "    - 'activity at t+n after that cell was active' - n is pre-determined\n",
    "- cell is part of higher order context... because it's a rolling input feed, i suppose\n",
    "## \"Temporal Unfolding\"\n",
    "Alternative way:\n",
    "\n",
    "\"One possible way to forecast beyond t+1 is to make the assumption that the t+1 prediction is right and feed it back into the system, just to see what it will predict at t+2. Theoretically you could repeat the process indefinitely although errors will grow the farther you go.\"\n",
    "\n",
    "\"\n",
    "Incidentally, this is something we usually refer to as “Temporal Unfolding”. It will most likely become a big part of HTM when we get around to performing actions (for example, combination of distal and apical input could be a mechanism for activating predictions). Dealing with uncertainty and error correction will also be a big part of temporal unfolding.\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so if I've got this correct, if we want to predict power consumption 15 hours in the future:\n",
    "- predict power(t+1), g\n",
    "    - for i in range(15):\n",
    "        - feed power(t+2+i) as input, add output to list\n",
    "        - store this list somewhere, since we want to return it when querying \"what are the 15 states after power(t)?\"\n",
    "- considerations:\n",
    "    - when do we want to predictor.infer() these extra 14 t-states?\n",
    "        - live, as in \"for every piece of data, return 15 predictions\"?\n",
    "        - post, as in \"once all the data's been eaten, re-loop through and get 14 more for each\"?\n",
    "            - in this case, do we keep (learn=False) for the post-extension-predictions?... probably!\n",
    "\n",
    "I feel like you'd want to do it live. Besides, when you've got the HTM deployed and data trickles in, you'd want to do it live or else you'd have to batch every so often which would get messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
